{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MYot7DJh9kk"
      },
      "source": [
        "See code at https://github.com/google-research/vision_transformer/\n",
        "\n",
        "See papers at\n",
        "\n",
        "- Vision Transformer: https://arxiv.org/abs/2010.11929\n",
        "- MLP-Mixer: https://arxiv.org/abs/2105.01601\n",
        "- How to train your ViT: https://arxiv.org/abs/2106.10270\n",
        "- When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations: https://arxiv.org/abs/2106.01548\n",
        "\n",
        "This Colab allows you to run the [JAX](https://jax.readthedocs.org) implementation of the Vision Transformer.\n",
        "\n",
        "If you just want to load a pre-trained checkpoint from a large repository and\n",
        "directly use it for inference, you probably want to go the other Colab\n",
        "\n",
        "https://colab.research.google.com/github/google-research/vision_transformer/blob/linen/vit_jax_augreg.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXhZm0kpPpH6"
      },
      "source": [
        "##### Copyright 2021 Google LLC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe6jvTCo1yQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KfmzfvFxPuk7",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOVCm4CnP1Do"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/google-research/vision_transformer/blob/master/vit_jax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyD76dm5JaeW"
      },
      "source": [
        "### Setup\n",
        "\n",
        "Needs to be executed once in every VM.\n",
        "\n",
        "The cell below downloads the code from Github and install necessary dependencies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "zZvI8OXt78sj",
        "outputId": "8651a3dd-4ea9-450a-8361-9cab35ecad27",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@markdown Select whether you would like to store data in your personal drive.\n",
        "#@markdown\n",
        "#@markdown If you select **yes**, you will need to authorize Colab to access\n",
        "#@markdown your personal drive\n",
        "#@markdown\n",
        "#@markdown If you select **no**, then any changes you make will diappear when\n",
        "#@markdown this Colab's VM restarts after some time of inactivity...\n",
        "use_gdrive = 'no'  #@param [\"yes\", \"no\"]\n",
        "\n",
        "if use_gdrive == 'yes':\n",
        "  from google.colab import drive\n",
        "  drive.mount('/gdrive')\n",
        "  root = '/gdrive/My Drive/vision_transformer_colab'\n",
        "  import os\n",
        "  if not os.path.isdir(root):\n",
        "    os.mkdir(root)\n",
        "  os.chdir(root)\n",
        "  print(f'\\nChanged CWD to \"{root}\"')\n",
        "else:\n",
        "  from IPython import display\n",
        "  display.display(display.HTML(\n",
        "      '<h1 style=\"color:red\">CHANGES NOT PERSISTED</h1>'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GeEy6gN71CDa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Clone repository and pull latest changes.\n",
        "![ -d vision_transformer ] || git clone --depth=1 https://github.com/google-research/vision_transformer\n",
        "!cd vision_transformer && git pull"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCN4d-GQJdU4",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!pip install -qr vision_transformer/vit_jax/requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcLBTSXuNjK6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BcnlJF7FKTfD",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Shows all available pre-trained models.\n",
        "!gsutil ls -lh gs://vit_models/imagenet*\n",
        "!gsutil ls -lh gs://vit_models/sam\n",
        "!gsutil ls -lh gs://mixer_models/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ztOhq_fzZyO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Download a pre-trained model.\n",
        "\n",
        "# Note: you can really choose any of the above, but this Colab has been tested\n",
        "# with the models of below selection...\n",
        "model_name = 'ViT-B_32'  #@param [\"ViT-B_32\", \"Mixer-B_16\"]\n",
        "\n",
        "if model_name.startswith('ViT'):\n",
        "  ![ -e \"$model_name\".npz ] || gsutil cp gs://vit_models/imagenet21k/\"$model_name\".npz .\n",
        "if model_name.startswith('Mixer'):\n",
        "  ![ -e \"$model_name\".npz ] || gsutil cp gs://mixer_models/imagenet21k/\"$model_name\".npz .\n",
        "\n",
        "import os\n",
        "assert os.path.exists(f'{model_name}.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EzOChfJeVrU",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Google Colab \"TPU\" runtimes are configured in \"2VM mode\", meaning that JAX\n",
        "# cannot see the TPUs because they're not directly attached. Instead we need to\n",
        "# setup JAX to communicate with a second machine that has the TPUs attached.\n",
        "import os\n",
        "if 'google.colab' in str(get_ipython()) and 'COLAB_TPU_ADDR' in os.environ:\n",
        "  import jax\n",
        "  import jax.tools.colab_tpu\n",
        "  jax.tools.colab_tpu.setup_tpu()\n",
        "  print('Connected to TPU.')\n",
        "else:\n",
        "  print('No TPU detected. Can be changed under \"Runtime/Change runtime type\".')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igqZ6qYNeHWo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from absl import logging\n",
        "import flax\n",
        "import jax\n",
        "from matplotlib import pyplot as plt\n",
        "import numpy as np\n",
        "import optax\n",
        "import tqdm\n",
        "\n",
        "logging.set_verbosity(logging.INFO)\n",
        "\n",
        "# Shows the number of available devices.\n",
        "# In a CPU/GPU runtime this will be a single device.\n",
        "# In a TPU runtime this will be 8 cores.\n",
        "jax.local_devices()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "9TuMn31fNj0T",
        "outputId": "1812d59e-676a-467d-cb22-b477f90ca106",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/configs/common.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/configs/models.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/checkpoint.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/input_pipeline.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/models.py\")"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "      ((filepath) => {{\n",
              "        if (!google.colab.kernel.accessAllowed) {{\n",
              "          return;\n",
              "        }}\n",
              "        google.colab.files.view(filepath);\n",
              "      }})(\"/content/vision_transformer/vit_jax/train.py\")"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Open some code files in a split editor on the right.\n",
        "# You can open more files in the file tab on the left.\n",
        "from google.colab import files\n",
        "files.view('vision_transformer/vit_jax/configs/common.py')\n",
        "files.view('vision_transformer/vit_jax/configs/models.py')\n",
        "files.view('vision_transformer/vit_jax/checkpoint.py')\n",
        "files.view('vision_transformer/vit_jax/input_pipeline.py')\n",
        "files.view('vision_transformer/vit_jax/models.py')\n",
        "files.view('vision_transformer/vit_jax/train.py')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjN0_b-YbaHu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Import files from repository.\n",
        "# Updating the files in the editor on the right will immediately update the\n",
        "# modules by re-importing them.\n",
        "\n",
        "import sys\n",
        "if './vision_transformer' not in sys.path:\n",
        "  sys.path.append('./vision_transformer')\n",
        "\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from vit_jax import checkpoint\n",
        "from vit_jax import input_pipeline\n",
        "from vit_jax import utils\n",
        "from vit_jax import models\n",
        "from vit_jax import train\n",
        "from vit_jax.configs import common as common_config\n",
        "from vit_jax.configs import models as models_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GojydzsXgknd",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Helper functions for images.\n",
        "\n",
        "labelnames = dict(\n",
        "  # https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "  cifar10=('airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'),\n",
        "  # https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "  cifar100=('apple', 'aquarium_fish', 'baby', 'bear', 'beaver', 'bed', 'bee', 'beetle', 'bicycle', 'bottle', 'bowl', 'boy', 'bridge', 'bus', 'butterfly', 'camel', 'can', 'castle', 'caterpillar', 'cattle', 'chair', 'chimpanzee', 'clock', 'cloud', 'cockroach', 'couch', 'crab', 'crocodile', 'cup', 'dinosaur', 'dolphin', 'elephant', 'flatfish', 'forest', 'fox', 'girl', 'hamster', 'house', 'kangaroo', 'computer_keyboard', 'lamp', 'lawn_mower', 'leopard', 'lion', 'lizard', 'lobster', 'man', 'maple_tree', 'motorcycle', 'mountain', 'mouse', 'mushroom', 'oak_tree', 'orange', 'orchid', 'otter', 'palm_tree', 'pear', 'pickup_truck', 'pine_tree', 'plain', 'plate', 'poppy', 'porcupine', 'possum', 'rabbit', 'raccoon', 'ray', 'road', 'rocket', 'rose', 'sea', 'seal', 'shark', 'shrew', 'skunk', 'skyscraper', 'snail', 'snake', 'spider', 'squirrel', 'streetcar', 'sunflower', 'sweet_pepper', 'table', 'tank', 'telephone', 'television', 'tiger', 'tractor', 'train', 'trout', 'tulip', 'turtle', 'wardrobe', 'whale', 'willow_tree', 'wolf', 'woman', 'worm')\n",
        ")\n",
        "def make_label_getter(dataset):\n",
        "  \"\"\"Returns a function converting label indices to names.\"\"\"\n",
        "  def getter(label):\n",
        "    if dataset in labelnames:\n",
        "      return labelnames[dataset][label]\n",
        "    return f'label={label}'\n",
        "  return getter\n",
        "\n",
        "def show_img(img, ax=None, title=None):\n",
        "  \"\"\"Shows a single image.\"\"\"\n",
        "  if ax is None:\n",
        "    ax = plt.gca()\n",
        "  ax.imshow(img[...])\n",
        "  ax.set_xticks([])\n",
        "  ax.set_yticks([])\n",
        "  if title:\n",
        "    ax.set_title(title)\n",
        "\n",
        "def show_img_grid(imgs, titles):\n",
        "  \"\"\"Shows a grid of images.\"\"\"\n",
        "  n = int(np.ceil(len(imgs)**.5))\n",
        "  _, axs = plt.subplots(n, n, figsize=(3 * n, 3 * n))\n",
        "  for i, (img, title) in enumerate(zip(imgs, titles)):\n",
        "    img = (img + 1) / 2  # Denormalize\n",
        "    show_img(img, axs[i // n][i % n], title)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZfK1vIIMmFz"
      },
      "source": [
        "### Load dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TSAVpYtP5VaE",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "dataset = 'cifar10'\n",
        "batch_size = 512\n",
        "config = common_config.with_dataset(common_config.get_config(), dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ruzdzpsMNhGm",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# For details about setting up datasets, see input_pipeline.py on the right.\n",
        "ds_train = input_pipeline.get_data_from_tfds(config=config, mode='train')\n",
        "ds_test = input_pipeline.get_data_from_tfds(config=config, mode='test')\n",
        "\n",
        "num_classes = input_pipeline.get_dataset_info(dataset, 'train')['num_classes']\n",
        "config.batch = batch_size\n",
        "config.pp.crop = 224\n",
        "\n",
        "del config  # Only needed to instantiate datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c-LfxOJdj8_",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Fetch a batch of test images for illustration purposes.\n",
        "batch = next(iter(ds_test.as_numpy_iterator()))\n",
        "# Note the shape : [num_local_devices, local_batch_size, h, w, c]\n",
        "batch['image'].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL0jQRBCgeJA",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Show some images with their labels.\n",
        "images, labels = batch['image'][0][:9], batch['label'][0][:9]\n",
        "titles = map(make_label_getter(dataset), labels.argmax(axis=1))\n",
        "show_img_grid(images, titles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFqi3h7yMEsB",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Same as above, but with train images.\n",
        "# Note how images are cropped/scaled differently.\n",
        "# Check out input_pipeline.get_data() in the editor at your right to see how the\n",
        "# images are preprocessed differently.\n",
        "batch = next(iter(ds_train.as_numpy_iterator()))\n",
        "images, labels = batch['image'][0][:9], batch['label'][0][:9]\n",
        "titles = map(make_label_getter(dataset), labels.argmax(axis=1))\n",
        "show_img_grid(images, titles)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehzbRTSN20E5"
      },
      "source": [
        "### Load pre-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kIeQWuyf4Ga",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "model_config = models_config.MODEL_CONFIGS[model_name]\n",
        "model_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMKr-4nK3DlT",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load model definition & initialize random parameters.\n",
        "# This also compiles the model to XLA (takes some minutes the first time).\n",
        "if model_name.startswith('Mixer'):\n",
        "  model = models.MlpMixer(num_classes=num_classes, **model_config)\n",
        "else:\n",
        "  model = models.VisionTransformer(num_classes=num_classes, **model_config)\n",
        "variables = jax.jit(lambda: model.init(\n",
        "    jax.random.PRNGKey(0),\n",
        "    # Discard the \"num_local_devices\" dimension of the batch for initialization.\n",
        "    batch['image'][0, :1],\n",
        "    train=False,\n",
        "), backend='cpu')()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIXjOEDkvAWM",
        "outputId": "fe49e572-d59d-49aa-de27-8929012a20db",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:absl:Inspect extra keys:\n",
            "{'pre_logits/bias', 'pre_logits/kernel'}\n",
            "INFO:absl:load_pretrained: drop-head variant\n"
          ]
        }
      ],
      "source": [
        "# Load and convert pretrained checkpoint.\n",
        "# This involves loading the actual pre-trained model results, but then also also\n",
        "# modifying the parameters a bit, e.g. changing the final layers, and resizing\n",
        "# the positional embeddings.\n",
        "# For details, refer to the code and to the methods of the paper.\n",
        "params = checkpoint.load_pretrained(\n",
        "    pretrained_path=f'{model_name}.npz',\n",
        "    init_params=variables['params'],\n",
        "    model_config=model_config,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQVKzhaR8o-J"
      },
      "source": [
        "### Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WB6ywRTY-LOa",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# So far, all our data is in the host memory. Let's now replicate the arrays\n",
        "# into the devices.\n",
        "# This will make every array in the pytree params become a ShardedDeviceArray\n",
        "# that has the same data replicated across all local devices.\n",
        "# For TPU it replicates the params in every core.\n",
        "# For a single GPU this simply moves the data onto the device.\n",
        "# For CPU it simply creates a copy.\n",
        "params_repl = flax.jax_utils.replicate(params)\n",
        "print('params.cls:', type(params['head']['bias']).__name__,\n",
        "      params['head']['bias'].shape)\n",
        "print('params_repl.cls:', type(params_repl['head']['bias']).__name__,\n",
        "      params_repl['head']['bias'].shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_unNxEZAK0Cu",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Then map the call to our model's forward pass onto all available devices.\n",
        "vit_apply_repl = jax.pmap(lambda params, inputs: model.apply(\n",
        "    dict(params=params), inputs, train=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgjFBUQ88p4z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "def get_accuracy(params_repl):\n",
        "  \"\"\"Returns accuracy evaluated on the test set.\"\"\"\n",
        "  good = total = 0\n",
        "  steps = input_pipeline.get_dataset_info(dataset, 'test')['num_examples'] // batch_size\n",
        "  for _, batch in zip(tqdm.trange(steps), ds_test.as_numpy_iterator()):\n",
        "    predicted = vit_apply_repl(params_repl, batch['image'])\n",
        "    is_same = predicted.argmax(axis=-1) == batch['label'].argmax(axis=-1)\n",
        "    good += is_same.sum()\n",
        "    total += len(is_same.flatten())\n",
        "  return good / total"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3qc7j0lv-F6-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Random performance without fine-tuning.\n",
        "get_accuracy(params_repl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxMdU_e5NeoT"
      },
      "source": [
        "### Fine-tune"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MI62dexw8mGo",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# 100 Steps take approximately 15 minutes in the TPU runtime.\n",
        "total_steps = 100\n",
        "warmup_steps = 5\n",
        "decay_type = 'cosine'\n",
        "grad_norm_clip = 1\n",
        "# This controls in how many forward passes the batch is split. 8 works well with\n",
        "# a TPU runtime that has 8 devices. 64 should work on a GPU. You can of course\n",
        "# also adjust the batch_size above, but that would require you to adjust the\n",
        "# learning rate accordingly.\n",
        "accum_steps = 8\n",
        "base_lr = 0.03"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzlfREb1ZHBY",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Check out train.make_update_fn in the editor on the right side for details.\n",
        "lr_fn = utils.create_learning_rate_schedule(total_steps, base_lr, decay_type, warmup_steps)\n",
        "# We use a momentum optimizer that uses half precision for state to save\n",
        "# memory. It als implements the gradient clipping.\n",
        "tx = optax.chain(\n",
        "    optax.clip_by_global_norm(grad_norm_clip),\n",
        "    optax.sgd(\n",
        "        learning_rate=lr_fn,\n",
        "        momentum=0.9,\n",
        "        accumulator_dtype='bfloat16',\n",
        "    ),\n",
        ")\n",
        "update_fn_repl = train.make_update_fn(\n",
        "    apply_fn=model.apply, accum_steps=accum_steps, tx=tx)\n",
        "opt_state = tx.init(params)\n",
        "opt_state_repl = flax.jax_utils.replicate(opt_state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTU7OmgjHb-G",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Initialize PRNGs for dropout.\n",
        "update_rng_repl = flax.jax_utils.replicate(jax.random.PRNGKey(0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zKn4IfUWHWPk",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "losses = []\n",
        "lrs = []\n",
        "# Completes in ~20 min on the TPU runtime.\n",
        "for step, batch in zip(\n",
        "    tqdm.trange(1, total_steps + 1),\n",
        "    ds_train.as_numpy_iterator(),\n",
        "):\n",
        "\n",
        "  params_repl, opt_state_repl, loss_repl, update_rng_repl = update_fn_repl(\n",
        "      params_repl, opt_state_repl, batch, update_rng_repl)\n",
        "  losses.append(loss_repl[0])\n",
        "  lrs.append(lr_fn(step))\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.figure()\n",
        "plt.plot(lrs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJhKAMhMI2D6",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Should be ~96.7% for Mixer-B/16 or 97.7% for ViT-B/32 on CIFAR10 (both @224)\n",
        "get_accuracy(params_repl)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-wIdj_qnbIM"
      },
      "source": [
        "### Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BrOSyiBsE-N-",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Download a pre-trained model.\n",
        "\n",
        "if model_name.startswith('Mixer'):\n",
        "  # Download model trained on imagenet2012\n",
        "  ![ -e \"$model_name\"_imagenet2012.npz ] || gsutil cp gs://mixer_models/imagenet1k/\"$model_name\".npz \"$model_name\"_imagenet2012.npz\n",
        "  model = models.MlpMixer(num_classes=1000, **model_config)\n",
        "else:\n",
        "  # Download model pre-trained on imagenet21k and fine-tuned on imagenet2012.\n",
        "  ![ -e \"$model_name\"_imagenet2012.npz ] || gsutil cp gs://vit_models/imagenet21k+imagenet2012/\"$model_name\".npz \"$model_name\"_imagenet2012.npz\n",
        "  model = models.VisionTransformer(num_classes=1000, **model_config)\n",
        "\n",
        "import os\n",
        "assert os.path.exists(f'{model_name}_imagenet2012.npz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TE7BoGkyoY7X",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Load and convert pretrained checkpoint.\n",
        "params = checkpoint.load(f'{model_name}_imagenet2012.npz')\n",
        "params['pre_logits'] = {}  # Need to restore empty leaf for Flax."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3TTypN0gkwO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get imagenet labels.\n",
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
        "imagenet_labels = dict(enumerate(open('ilsvrc2012_wordnet_lemmas.txt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQwomPOopDlr",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Get a random picture with the correct dimensions.\n",
        "resolution = 224 if model_name.startswith('Mixer') else 384\n",
        "!wget https://picsum.photos/$resolution -O picsum.jpg\n",
        "import PIL\n",
        "img = PIL.Image.open('picsum.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrvwNAGJshzb",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Predict on a batch with a single item (note very efficient TPU usage...)\n",
        "logits, = model.apply(dict(params=params), (np.array(img) / 128 - 1)[None, ...], train=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "64hwCdaehs42",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "preds = np.array(jax.nn.softmax(logits))\n",
        "for idx in preds.argsort()[:-11:-1]:\n",
        "  print(f'{preds[idx]:.5f} : {imagenet_labels[idx]}', end='')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
